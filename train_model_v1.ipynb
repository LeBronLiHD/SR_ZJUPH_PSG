{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcb1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import params\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b416c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load .edf -> .\\data\\PSGData1_Hang7\\20190917-T3-93135.edf\n",
      "Extracting EDF parameters from E:\\ZJU_Research\\SR_ZJUPH_PSG\\data\\PSGData1_Hang7\\20190917-T3-93135.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ZJU_Research\\SR_ZJUPH_PSG\\utils.py:27: RuntimeWarning: Physical range is not defined in following channels:\n",
      "Thor, Abdo\n",
      "  raw_train = mne.io.read_raw_edf(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186 36441088\n",
      "Normalizing ... Current Dimension -> 0\n",
      "Normalizing ... Current Dimension -> 1\n",
      "Normalizing ... Current Dimension -> 2\n",
      "Normalizing ... Current Dimension -> 3\n",
      "Normalizing ... Current Dimension -> 4\n",
      "Normalizing ... Current Dimension -> 5\n",
      "Normalizing ... Current Dimension -> 6\n",
      "Normalizing ... Current Dimension -> 7\n",
      "Normalizing ... Current Dimension -> 8\n",
      "Normalizing ... Current Dimension -> 9\n",
      "Normalizing ... Current Dimension -> 10\n",
      "Normalizing ... Current Dimension -> 11\n",
      "Normalizing ... Current Dimension -> 12\n",
      "Normalizing ... Current Dimension -> 13\n",
      "Normalizing ... Current Dimension -> 14\n",
      "Normalizing ... Current Dimension -> 15\n",
      "Normalizing ... Current Dimension -> 16\n",
      "Normalizing ... Current Dimension -> 17\n",
      "Normalizing ... Current Dimension -> 18\n",
      "Normalizing ... Current Dimension -> 19\n",
      "Normalizing ... Current Dimension -> 20\n",
      "Return data, np.std(data) is too Low! len data -> 36441088\n",
      "Normalizing ... Current Dimension -> 21\n",
      "Normalizing ... Current Dimension -> 22\n",
      "Return data, np.std(data) is too Low! len data -> 36441088\n",
      "Normalization Done!\n",
      "Generating Train&Test ... Current Index -> 0\n",
      "Generating Train&Test ... Current Index -> 50\n",
      "Generating Train&Test ... Current Index -> 100\n",
      "Generating Train&Test ... Current Index -> 150\n",
      "Generating Train&Test ... Current Index -> 200\n",
      "Generating Train&Test ... Current Index -> 250\n",
      "Generating Train&Test ... Current Index -> 300\n",
      "Generating Train&Test ... Current Index -> 350\n",
      "Generating Train&Test ... Current Index -> 400\n",
      "Generating Train&Test ... Current Index -> 450\n",
      "Generating Train&Test ... Current Index -> 500\n",
      "Generating Train&Test ... Current Index -> 550\n",
      "Generating Train&Test ... Current Index -> 600\n",
      "Generating Train&Test ... Current Index -> 650\n",
      "Generating Train&Test ... Current Index -> 700\n",
      "Generating Train&Test ... Current Index -> 750\n",
      "Generating Train&Test ... Current Index -> 800\n",
      "Generating Train&Test ... Current Index -> 850\n",
      "Generating Train&Test ... Current Index -> 900\n",
      "Generating Train&Test ... Current Index -> 950\n",
      "Generating Train&Test ... Current Index -> 1000\n",
      "Generating Train&Test ... Current Index -> 1050\n",
      "Generating Train&Test ... Current Index -> 1100\n",
      "Generating Train&Test ... Current Index -> 1150\n",
      "Load .edf -> .\\data\\PSGData2_Hang7\\20190924-T2-93329.edf\n",
      "Extracting EDF parameters from E:\\ZJU_Research\\SR_ZJUPH_PSG\\data\\PSGData2_Hang7\\20190924-T2-93329.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ZJU_Research\\SR_ZJUPH_PSG\\utils.py:27: RuntimeWarning: Physical range is not defined in following channels:\n",
      "Thor, Abdo\n",
      "  raw_train = mne.io.read_raw_edf(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168 35885056\n",
      "Normalizing ... Current Dimension -> 0\n",
      "Normalizing ... Current Dimension -> 1\n",
      "Normalizing ... Current Dimension -> 2\n",
      "Normalizing ... Current Dimension -> 3\n",
      "Normalizing ... Current Dimension -> 4\n",
      "Normalizing ... Current Dimension -> 5\n",
      "Normalizing ... Current Dimension -> 6\n",
      "Normalizing ... Current Dimension -> 7\n",
      "Normalizing ... Current Dimension -> 8\n",
      "Normalizing ... Current Dimension -> 9\n",
      "Normalizing ... Current Dimension -> 10\n",
      "Normalizing ... Current Dimension -> 11\n",
      "Normalizing ... Current Dimension -> 12\n",
      "Normalizing ... Current Dimension -> 13\n",
      "Normalizing ... Current Dimension -> 14\n",
      "Normalizing ... Current Dimension -> 15\n",
      "Normalizing ... Current Dimension -> 16\n",
      "Normalizing ... Current Dimension -> 17\n",
      "Normalizing ... Current Dimension -> 18\n",
      "Normalizing ... Current Dimension -> 19\n",
      "Normalizing ... Current Dimension -> 20\n",
      "Normalizing ... Current Dimension -> 21\n",
      "Normalizing ... Current Dimension -> 22\n",
      "Normalization Done!\n",
      "Generating Train&Test ... Current Index -> 0\n",
      "Generating Train&Test ... Current Index -> 50\n",
      "Generating Train&Test ... Current Index -> 100\n",
      "Generating Train&Test ... Current Index -> 150\n",
      "Generating Train&Test ... Current Index -> 200\n",
      "Generating Train&Test ... Current Index -> 250\n",
      "Generating Train&Test ... Current Index -> 300\n",
      "Generating Train&Test ... Current Index -> 350\n",
      "Generating Train&Test ... Current Index -> 400\n",
      "Generating Train&Test ... Current Index -> 450\n",
      "Generating Train&Test ... Current Index -> 500\n",
      "Generating Train&Test ... Current Index -> 550\n",
      "Generating Train&Test ... Current Index -> 600\n",
      "Generating Train&Test ... Current Index -> 650\n",
      "Generating Train&Test ... Current Index -> 700\n",
      "Generating Train&Test ... Current Index -> 750\n",
      "Generating Train&Test ... Current Index -> 800\n",
      "Generating Train&Test ... Current Index -> 850\n",
      "Generating Train&Test ... Current Index -> 900\n",
      "Generating Train&Test ... Current Index -> 950\n",
      "Generating Train&Test ... Current Index -> 1000\n",
      "Generating Train&Test ... Current Index -> 1050\n",
      "Generating Train&Test ... Current Index -> 1100\n",
      "Generating Train&Test ... Current Index -> 1150\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.15 GiB for an array with shape (1168, 23, 30726) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ZJU_Research\\SR_ZJUPH_PSG\\data_loader.py:67\u001b[0m, in \u001b[0;36mget_train_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mPATIENTS:\n\u001b[1;32m---> 67\u001b[0m     cur_x_train, cur_x_test, cur_y_train, cur_y_test \u001b[38;5;241m=\u001b[39m \u001b[43m_get_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     x_train\u001b[38;5;241m.\u001b[39mextend(cur_x_train)\n\u001b[0;32m     69\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mextend(cur_y_train)\n",
      "File \u001b[1;32mE:\\ZJU_Research\\SR_ZJUPH_PSG\\data_loader.py:54\u001b[0m, in \u001b[0;36m_get_train_test\u001b[1;34m(patient)\u001b[0m\n\u001b[0;32m     52\u001b[0m txt_data \u001b[38;5;241m=\u001b[39m txt_data[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(txt_data), \u001b[38;5;28mlen\u001b[39m(edf_data))\n\u001b[1;32m---> 54\u001b[0m ori_x, ori_y \u001b[38;5;241m=\u001b[39m \u001b[43m_generate_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_normal_edf\u001b[49m\u001b[43m(\u001b[49m\u001b[43medf_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _split_train_test(ori_x, ori_y)\n",
      "File \u001b[1;32mE:\\ZJU_Research\\SR_ZJUPH_PSG\\data_loader.py:46\u001b[0m, in \u001b[0;36m_generate_train_test\u001b[1;34m(edf_data, txt_data)\u001b[0m\n\u001b[0;32m     44\u001b[0m         ori_x_unit\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(edf_data[j][\u001b[38;5;28mround\u001b[39m(params\u001b[38;5;241m.\u001b[39mUNIT_LENGTH_F \u001b[38;5;241m*\u001b[39m j):\u001b[38;5;28mround\u001b[39m(params\u001b[38;5;241m.\u001b[39mUNIT_LENGTH_F \u001b[38;5;241m*\u001b[39m j) \u001b[38;5;241m+\u001b[39m params\u001b[38;5;241m.\u001b[39mUNIT_LENGTH]\u001b[38;5;241m.\u001b[39mT))\n\u001b[0;32m     45\u001b[0m     ori_x\u001b[38;5;241m.\u001b[39mappend(ori_x_unit)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mori_x\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(ori_y)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.15 GiB for an array with shape (1168, 23, 30726) and data type float64"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data_loader.get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab4120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ann -y\n",
    "!conda install -c conda-forge ann_visualizer -y\n",
    "!conda install graphviz -y\n",
    "!conda install -c anaconda python-graphviz -y\n",
    "!conda install -c anaconda pydot -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefd356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import cnn_1, cnn_2, cnn_lstm, cnn_crf, resnet_101v2, efficient_net_v2l, vgg_19\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorboard\n",
    "import datetime\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7616034",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST.append(cnn_1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438aec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST.append(cnn_2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773947c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST.append(cnn_lstm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ab3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST.append(cnn_crf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_LIST = [\"cnn_1\", \"cnn_2\", \"cnn_lstm\", \"cnn_crf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_LIST, TRAINED_MODEL = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630f090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(MODEL_LIST[0], to_file=\"./images/\" + NAME_LIST[0] + \".png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f43e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(MODEL_LIST[1], to_file=\"./images/\" + NAME_LIST[1] + \".png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6451e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(MODEL_LIST[2], to_file=\"./images/\" + NAME_LIST[2] + \".png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039a825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(MODEL_LIST[3], to_file=\"./images/\" + NAME_LIST[3] + \".png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6640bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISABLE_GPU = False\n",
    "if DISABLE_GPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(file_path, model_name):\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20, verbose=1)\n",
    "    redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=2)\n",
    "    log_dir = \"logs/\" + str(model_name) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tfboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks_list = [checkpoint, early, redonplat, tfboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name):\n",
    "    file_path = './models/' + name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5'\n",
    "    callbacks_list = get_callbacks(file_path, name)\n",
    "    opt = Adam(learning_rate=params.LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    history = model.fit((x_train, y_train),\n",
    "                    epochs=params.EPOCH_NUM,\n",
    "                    batch_size=params.BATCH_SIZE,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=params.VERBOSE,\n",
    "                    callbacks=callbacks_list)\n",
    "    model = load_model(file_path)\n",
    "    test_loss, test_acc = model.evaluate((x_test, y_test), steps=len(y_test), verbose=1)\n",
    "    print('Loss: ', test_loss)\n",
    "    print('Accuracy: ', test_acc) \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaac142",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(MODEL_LIST)):\n",
    "    history, trained_model = train_model(MODEL_LIST[i], NAME_LIST[i])\n",
    "    HIST_LIST.append(history)\n",
    "    TRAINED_MODEL.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4661267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(matrix, classes, name):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    cmap = \"YlGnBu\"\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', ax=ax, cmap=cmap);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "    plt.savefig('./images/' + name + 'con_mat.png')\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(classes); \n",
    "    ax.yaxis.set_ticklabels(classes[::-1]);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382590d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(model, name, plot=True, labels=[0, 1, 2, 3, 4]):\n",
    "    matrix = confusion_matrix(predictions, labels)\n",
    "    print(matrix)\n",
    "    print('\\n')\n",
    "\n",
    "    f1 = f1_score(predictions, labels, average='weighted')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('\\n')\n",
    "    \n",
    "    print(classification_report(predictions, labels, target_names=classes))\n",
    "    \n",
    "    if plot:\n",
    "        confusion_matrix_plot(matrix, labels, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3397185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_f1(model, history, name):\n",
    "    cal_score(model, name, plot=True)\n",
    "    tr_plot(history, 0)\n",
    "    print(\"==\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c97bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(HIST_LIST)):\n",
    "    plot_and_f1(TRAINED_MODEL[i], HIST_LIST[i], NAME_LIST[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
